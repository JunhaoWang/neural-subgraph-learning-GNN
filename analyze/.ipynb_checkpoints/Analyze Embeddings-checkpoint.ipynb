{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding analysis\n",
    "\n",
    "In this notebook we visualize the embeddings learned by the encoder. Run this notebook after training a model via the `encoder.train` module.\n",
    "\n",
    "We start by importing needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset wn18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.82s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict\n",
    "from itertools import permutations\n",
    "from queue import PriorityQueue\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import torch.multiprocessing as mp\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from common import data\n",
    "from common import models\n",
    "from common import utils\n",
    "from subgraph_matching.config import parse_encoder\n",
    "\n",
    "# Now we load the model and a dataset to analyze embeddings on, here ENZYMES.\n",
    "\n",
    "from subgraph_matching.train import make_data_source\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "utils.parse_optimizer(parser)\n",
    "parse_encoder(parser)\n",
    "args = parser.parse_args(\"\")\n",
    "args.model_path = os.path.join(\"..\", args.model_path)\n",
    "\n",
    "print(\"Using dataset {}\".format(args.dataset))\n",
    "model = models.OrderEmbedder(1, args.hidden_dim, args)\n",
    "model.to(utils.get_device())\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(args.model_path,\n",
    "    map_location=utils.get_device()))\n",
    "\n",
    "train, test, task = data.load_dataset(\"wn18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs = []\n",
    "for i in range(100):\n",
    "    graph, neigh = utils.sample_neigh(train, random.randint(3, 29))\n",
    "    motifs.append(graph.subgraph(neigh))\n",
    "\n",
    "batch = utils.batch_nx_graphs(motifs)\n",
    "embs = model.emb_model(batch).detach().cpu().numpy()\n",
    "\n",
    "max_n_edges = max([len(m.edges) for m in motifs])\n",
    "max_n_nodes = max([len(m) for m in motifs])\n",
    "\n",
    "# pca of all embeddings\n",
    "pca = PCA(n_components=2).fit(embs)\n",
    "xs, ys = zip(*pca.transform(embs))\n",
    "cmap = cm.get_cmap(\"viridis\", 12)\n",
    "colors = [cmap(len(m.edges)/max_n_edges) for m in motifs]\n",
    "#colors = [cmap(len(m.nodes)/max_n_nodes) for m in motifs]\n",
    "plt.scatter(xs, ys, color=colors)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Next we investigate a bit the dependence of embeddings on graph size, in terms of both number of nodes and number of edges. We see that the minimum embedding norm for each graph size scales roughly with size, which makes sense given the margin-based objective. Additionally, there is some variation within each graph size, suggesting the model picks up on more than simply size-based cues. There is more variation when measuring graph size by number of edges, which makes sense given the node-induced subgraph setting.\n",
    "\n",
    "sorted_motifs = [motifs[i] for i in np.argsort(xs)]\n",
    "\n",
    "for i in sorted_motifs:\n",
    "    colors = [i[u][v]['edge_type'] for u,v in i.edges]\n",
    "    nx.draw(i, edge_color=colors)\n",
    "    plt.show()\n",
    "\n",
    "# plot norm vs number of nodes\n",
    "xs, ys = [len(m) for m in motifs], [np.linalg.norm(emb) for emb in embs]\n",
    "plt.scatter(xs, ys)\n",
    "plt.xlabel(\"Graph size (number of nodes)\")\n",
    "plt.ylabel(\"Embedding norm\")\n",
    "plt.show()\n",
    "\n",
    "# plot norm vs number of edges\n",
    "xs, ys = [len(m.edges) for m in motifs], [np.linalg.norm(emb) for emb in embs]\n",
    "plt.scatter(xs, ys)\n",
    "plt.xlabel(\"Graph size (number of edges)\")\n",
    "plt.ylabel(\"Embedding norm\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# heatmap of node size vs edge size vs embedding norm\n",
    "a = defaultdict(lambda: defaultdict(list))\n",
    "for m, emb in zip(motifs, embs):\n",
    "    a[len(m)][len(m.edges)].append(np.linalg.norm(emb))\n",
    "a_mat = np.zeros((max_n_edges, max_n_nodes))\n",
    "for i in range(max_n_nodes):\n",
    "    for j in range(max_n_edges):\n",
    "        a_mat[j][i] = np.mean(a[i][j] if a[i][j] else 0)\n",
    "ax = sns.heatmap(a_mat, linewidth=0)\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel(\"Number of nodes in graph\")\n",
    "plt.ylabel(\"Number of edges in graph\")\n",
    "plt.title(\"Average embedding norm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset wn18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.50s/it]\n",
      "1it [00:02,  2.49s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in _update_attributes, number of edges in Graph G must be larger than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a2a72b9cba8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         pos_a, pos_b, neg_a, neg_b, _ = data_source.gen_batch(batch_target,\n\u001b[0;32m---> 70\u001b[0;31m                                                               batch_neg_target, batch_neg_query, True, one_small=True)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mpos_a_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_b_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_a_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_b_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_a_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_b_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_a_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_b_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kg_project/neural-subgraph-learning-GNN/common/data.py\u001b[0m in \u001b[0;36mgen_batch\u001b[0;34m(self, a, b, c, train, max_size, min_size, seed, filter_negs, sample_method, one_small)\u001b[0m\n\u001b[1;32m    414\u001b[0m             self.node_anchored else None)\n\u001b[1;32m    415\u001b[0m         pos_b = utils.batch_nx_graphs(pos_b, anchors=pos_b_anchors if\n\u001b[0;32m--> 416\u001b[0;31m             self.node_anchored else None)\n\u001b[0m\u001b[1;32m    417\u001b[0m         neg_a = utils.batch_nx_graphs(neg_a, anchors=neg_a_anchors if\n\u001b[1;32m    418\u001b[0m             self.node_anchored else None)\n",
      "\u001b[0;32m~/kg_project/neural-subgraph-learning-GNN/common/utils.py\u001b[0m in \u001b[0;36mbatch_nx_graphs\u001b[0;34m(graphs, anchors)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"edge_feature\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_data_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGraphDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg/lib/python3.6/site-packages/deepsnap/dataset.py\u001b[0m in \u001b[0;36mlist_to_graphs\u001b[0;34m(G_list)\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdeepsnap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \"\"\"\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg/lib/python3.6/site-packages/deepsnap/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdeepsnap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \"\"\"\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg/lib/python3.6/site-packages/deepsnap/graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, G, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_positive_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg/lib/python3.6/site-packages/deepsnap/graph.py\u001b[0m in \u001b[0;36m_update_tensors\u001b[0;34m(self, init)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mUpdate\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0medge\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \"\"\"\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kg/lib/python3.6/site-packages/deepsnap/graph.py\u001b[0m in \u001b[0;36m_update_attributes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             raise ValueError(\n\u001b[0;32m--> 427\u001b[0;31m                 \u001b[0;34m\"in _update_attributes, number of edges in Graph \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m                 \u001b[0;34m\"G must be larger than 0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: in _update_attributes, number of edges in Graph G must be larger than 0"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict\n",
    "from itertools import permutations\n",
    "from queue import PriorityQueue\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import torch.multiprocessing as mp\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from common import data\n",
    "from common import models\n",
    "from common import utils\n",
    "from subgraph_matching.config import parse_encoder\n",
    "\n",
    "# Now we load the model and a dataset to analyze embeddings on, here ENZYMES.\n",
    "\n",
    "from subgraph_matching.train import make_data_source\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "utils.parse_optimizer(parser)\n",
    "parse_encoder(parser)\n",
    "args = parser.parse_args(\"\")\n",
    "args.model_path = os.path.join(\"..\", args.model_path)\n",
    "\n",
    "print(\"Using dataset {}\".format(args.dataset))\n",
    "model = models.OrderEmbedder(1, args.hidden_dim, args)\n",
    "model.to(utils.get_device())\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(args.model_path,\n",
    "    map_location=utils.get_device()))\n",
    "\n",
    "train, test, task = data.load_dataset(\"wn18\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "done = False\n",
    "train_accs = []\n",
    "while not done:\n",
    "    data_source = make_data_source(args)\n",
    "    loaders = data_source.gen_data_loaders(args.eval_interval *\n",
    "                                           args.batch_size, args.batch_size, train=True)\n",
    "    for batch_target, batch_neg_target, batch_neg_query in zip(*loaders):\n",
    "\n",
    "        pos_a, pos_b, neg_a, neg_b, _ = data_source.gen_batch(batch_target,\n",
    "                                                              batch_neg_target, batch_neg_query, True, one_small=True)\n",
    "\n",
    "        pos_a_g, pos_b_g, neg_a_g, neg_b_g, pos_a_anchors, pos_b_anchors, neg_a_anchors, neg_b_anchors = _\n",
    "\n",
    "        emb_pos_a, emb_pos_b = model.emb_model(pos_a), model.emb_model(pos_b)\n",
    "        emb_neg_a, emb_neg_b = model.emb_model(neg_a), model.emb_model(neg_b)\n",
    "\n",
    "        emb_as = torch.cat((emb_pos_a, emb_neg_a), dim=0)\n",
    "        emb_bs = torch.cat((emb_pos_b, emb_neg_b), dim=0)\n",
    "        labels = torch.tensor([1] * pos_a.num_graphs + [0] * neg_a.num_graphs).to(\n",
    "            utils.get_device())\n",
    "        intersect_embs = None\n",
    "        pred = model(emb_as, emb_bs)\n",
    "        loss = model.criterion(pred, intersect_embs, labels)\n",
    "\n",
    "        if args.method_type == \"order\":\n",
    "            with torch.no_grad():\n",
    "                pred = model.predict(pred)\n",
    "            model.clf_model.zero_grad()\n",
    "            pred = model.clf_model(pred.unsqueeze(1))\n",
    "            criterion = nn.NLLLoss()\n",
    "            clf_loss = criterion(pred, labels)\n",
    "\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        acc = torch.mean((pred == labels).type(torch.float))\n",
    "        train_loss = loss.item()\n",
    "        train_acc = acc.item()\n",
    "\n",
    "        print(train_acc)\n",
    "\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        failed_0 = np.argwhere(((pred != labels) & (labels == 0)).detach().cpu()).reshape(-1).numpy()\n",
    "        success_0 = np.argwhere(((pred == labels) & (labels == 0)).detach().cpu()).reshape(-1).numpy()\n",
    "\n",
    "        a_g = pos_a_g + neg_a_g\n",
    "        b_g = pos_b_g + neg_b_g\n",
    "\n",
    "        a_g_failed = [a_g[i] for i in failed_0]\n",
    "        b_g_failed = [b_g[i] for i in failed_0]\n",
    "        lab_failed = [labels[i].item() for i in failed_0]\n",
    "\n",
    "        a_g_suc = [a_g[i] for i in success_0]\n",
    "        b_g_suc = [b_g[i] for i in success_0]\n",
    "        lab_suc = [labels[i].item() for i in success_0]\n",
    "\n",
    "\n",
    "        break\n",
    "        if len(train_accs) > 1:\n",
    "            break\n",
    "    break\n",
    "    if len(train_accs) > 1:\n",
    "        break\n",
    "c = -1\n",
    "for i in range(len(pos_a_g)):\n",
    "    c += 1\n",
    "    a = pos_a_g[i]\n",
    "    b = pos_b_g[i]\n",
    "\n",
    "    colors = [a[u][v]['edge_type'] for u, v in a.edges]\n",
    "    lay_a = nx.spring_layout(a)\n",
    "    if 0 in a.nodes:\n",
    "        nx.draw(a, edge_color=colors, node_color=[1] + [0] * (len(a) - 1), pos=lay_a)\n",
    "    else:\n",
    "        nx.draw(a, edge_color=colors, pos=lay_a)\n",
    "    plt.title(F'failed negative a {c}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    colors = [b[u][v]['edge_type'] for u, v in b.edges]\n",
    "    if 0 in b.nodes:\n",
    "        nx.draw(b, edge_color=colors, node_color=[1] + [0] * (len(b) - 1), pos=lay_a)\n",
    "    else:\n",
    "        nx.draw(b, edge_color=colors, pos=lay_a)\n",
    "    plt.title(F'failed negative b {c}')\n",
    "    plt.show()\n",
    "\n",
    "    print('-' * 50)\n",
    "c = -1\n",
    "for i in range(len(lab_failed)):\n",
    "    c += 1\n",
    "    a = a_g_failed[i]\n",
    "    b = b_g_failed[i]\n",
    "\n",
    "    colors = [b[u][v]['edge_type'] for u, v in b.edges]\n",
    "\n",
    "    if 0 in b.nodes:\n",
    "        nx.draw(b, edge_color=colors)\n",
    "    else:\n",
    "        nx.draw(b, edge_color=colors)\n",
    "    plt.title(F'success negative b {c}')\n",
    "    plt.show()\n",
    "\n",
    "    colors = [a[u][v]['edge_type'] for u, v in a.edges]\n",
    "\n",
    "    if 0 in a.nodes:\n",
    "        nx.draw(a, edge_color=colors)\n",
    "    else:\n",
    "        nx.draw(a, edge_color=colors)\n",
    "\n",
    "    plt.title(F'success negative a {c}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
